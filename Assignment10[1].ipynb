{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6ef81a-b8b0-45d1-b5a1-e06df542a56d",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Q1. Write a unique paragraph (5â€“6 sentences) about your favorite topic.\n",
        "\n",
        "Paragraph:\n",
        "Space exploration has always fascinated humanity and pushed the boundaries of science. Missions to Mars and the Moon have expanded our understanding of the universe. Satellites orbiting the Earth enable communication, navigation, and weather prediction. Advanced telescopes like James Webb allow us to peer into distant galaxies. Private companies like SpaceX are making space travel more accessible and inspiring future generations.\n",
        "\n",
        "Q1 Solution:\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "paragraph = \"\"\"Space exploration has always fascinated humanity and pushed the boundaries of science. Missions to Mars and the Moon have expanded our understanding of the universe. Satellites orbiting the Earth enable communication, navigation, and weather prediction. Advanced telescopes like James Webb allow us to peer into distant galaxies. Private companies like SpaceX are making space travel more accessible and inspiring future generations.\"\"\"\n",
        "\n",
        "# 1. Lowercase and remove punctuation\n",
        "lowered = paragraph.lower()\n",
        "cleaned = re.sub(r'[^\\w\\s]', '', lowered)\n",
        "\n",
        "# 2. Tokenization\n",
        "word_tokens = word_tokenize(cleaned)\n",
        "sent_tokens = sent_tokenize(paragraph)\n",
        "\n",
        "# 3. Split vs word_tokenize\n",
        "split_tokens = cleaned.split()\n",
        "print(\"Split tokens:\", split_tokens[:10])\n",
        "print(\"Word_tokenize tokens:\", word_tokens[:10])\n",
        "\n",
        "# 4. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "# 5. Word frequency (excluding stopwords)\n",
        "freq_dist = Counter(filtered_words)\n",
        "print(\"Word Frequency Distribution (no stopwords):\", freq_dist)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
